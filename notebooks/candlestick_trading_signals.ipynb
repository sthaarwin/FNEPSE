{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cceb10",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras import layers, models #type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa169c",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/merged-stock-data.csv\")\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "def add_advanced_technical_features(df):\n",
    "    \"\"\"Add advanced technical indicators used by professional traders\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic technical features\n",
    "    df['ma_5'] = df['close'].rolling(5).mean()\n",
    "    df['ma_20'] = df['close'].rolling(20).mean()\n",
    "    df['price_change'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['close'].rolling(10).std()\n",
    "    df['hl_spread'] = (df['high'] - df['low']) / df['close']\n",
    "    df['oc_spread'] = (df['close'] - df['open']) / df['open']\n",
    "    \n",
    "    # Advanced momentum indicators\n",
    "    df['rsi'] = calculate_rsi(df['close'], 14)\n",
    "    df['macd'], df['macd_signal'] = calculate_macd(df['close'])\n",
    "    df['bb_upper'], df['bb_lower'], df['bb_width'] = calculate_bollinger_bands(df['close'])\n",
    "    \n",
    "    # Volume-based indicators (if volume available)\n",
    "    if 'Volume' in df.columns:\n",
    "        df['volume_sma'] = df['Volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['Volume'] / df['volume_sma']\n",
    "        df['vwap'] = calculate_vwap(df)\n",
    "    \n",
    "    # Support/Resistance levels\n",
    "    df['support'], df['resistance'] = calculate_support_resistance(df)\n",
    "    \n",
    "    # Market structure\n",
    "    df['higher_high'] = calculate_higher_highs(df['high'])\n",
    "    df['lower_low'] = calculate_lower_lows(df['low'])\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD indicator\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    macd_signal = macd.ewm(span=signal).mean()\n",
    "    return macd, macd_signal\n",
    "\n",
    "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    sma = prices.rolling(period).mean()\n",
    "    std = prices.rolling(period).std()\n",
    "    upper = sma + (std * std_dev)\n",
    "    lower = sma - (std * std_dev)\n",
    "    width = (upper - lower) / sma\n",
    "    return upper, lower, width\n",
    "\n",
    "def calculate_vwap(df):\n",
    "    \"\"\"Calculate Volume Weighted Average Price\"\"\"\n",
    "    if 'Volume' in df.columns:\n",
    "        return (df['close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "def calculate_support_resistance(df, window=20):\n",
    "    \"\"\"Calculate support and resistance levels\"\"\"\n",
    "    support = df['low'].rolling(window).min()\n",
    "    resistance = df['high'].rolling(window).max()\n",
    "    return support, resistance\n",
    "\n",
    "def calculate_higher_highs(highs, period=5):\n",
    "    \"\"\"Detect higher highs pattern\"\"\"\n",
    "    return highs > highs.shift(1).rolling(period).max()\n",
    "\n",
    "def calculate_lower_lows(lows, period=5):\n",
    "    \"\"\"Detect lower lows pattern\"\"\"\n",
    "    return lows < lows.shift(1).rolling(period).min()\n",
    "\n",
    "print(\"Adding advanced technical features...\")\n",
    "df = add_advanced_technical_features(df)\n",
    "\n",
    "feature_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\",\n",
    "    \"ma_5\", \"ma_20\", \"price_change\", \"volatility\", \"hl_spread\", \"oc_spread\"\n",
    "]\n",
    "\n",
    "print(f\"Enhanced features: {len(feature_cols)} features\")\n",
    "print(f\"Data shape after feature engineering: {df.shape}\")\n",
    "\n",
    "x_raw = df[feature_cols].values\n",
    "scaler = MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x_raw)\n",
    "print(\"Features scaled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41febc13",
   "metadata": {},
   "source": [
    "# 3. Load Saved Pattern Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aafdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_model = load_model(\"../models/candlestick_cnn_lstm.h5\")\n",
    "\n",
    "lookback = 20\n",
    "\n",
    "x_seq = []\n",
    "for i in range(lookback, len(x_scaled)):\n",
    "    x_seq.append(x_scaled[i-lookback:i])\n",
    "\n",
    "x_seq = np.array(x_seq)\n",
    "\n",
    "patterns_preds = np.argmax(pattern_model.predict(x_seq), axis=1)\n",
    "print(\"predicted pattern shape : \", patterns_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d86360",
   "metadata": {},
   "source": [
    "# 4. Create Trading Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2072716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_advanced_trading_signals(df, lookback):\n",
    "    \"\"\"Generate sophisticated trading signals using multiple criteria\"\"\"\n",
    "    \n",
    "    # Multi-timeframe analysis - ensure we're not using future data\n",
    "    returns_1d = df[\"close\"].pct_change().values\n",
    "    returns_5d = df[\"close\"].pct_change(periods=5).values\n",
    "    returns_20d = df[\"close\"].pct_change(periods=20).values\n",
    "    \n",
    "    # Trim arrays to match lookback\n",
    "    returns_1d = returns_1d[lookback:]\n",
    "    returns_5d = returns_5d[lookback:]\n",
    "    returns_20d = returns_20d[lookback:]\n",
    "    \n",
    "    # Use more conservative thresholds to reduce noise\n",
    "    vol_threshold = np.percentile(df['volatility'].values[lookback:], 75)  # More conservative\n",
    "    volatility = df['volatility'].values[lookback:]\n",
    "    vol_regime = volatility > vol_threshold\n",
    "    \n",
    "    # More conservative signal criteria\n",
    "    y_trading = np.ones(len(returns_1d), dtype=int)  # Default Hold\n",
    "    \n",
    "    # Strong Buy signals - more restrictive\n",
    "    strong_buy = (\n",
    "        (returns_1d > np.percentile(returns_1d, 85)) &  # Very strong short-term momentum\n",
    "        (returns_5d > np.percentile(returns_5d, 70)) &  # Strong medium-term trend\n",
    "        (returns_20d > np.percentile(returns_20d, 60)) &  # Positive long-term trend\n",
    "        (~vol_regime)  # Low volatility environment\n",
    "    )\n",
    "    \n",
    "    # Strong Sell signals - more restrictive\n",
    "    strong_sell = (\n",
    "        (returns_1d < np.percentile(returns_1d, 15)) &  # Very strong negative momentum\n",
    "        (returns_5d < np.percentile(returns_5d, 30)) &  # Strong negative medium-term trend\n",
    "        (returns_20d < np.percentile(returns_20d, 40)) &  # Negative long-term trend\n",
    "        (~vol_regime)  # Low volatility environment\n",
    "    )\n",
    "    \n",
    "    # Apply signals - be more conservative\n",
    "    y_trading[strong_buy] = 2  # Buy\n",
    "    y_trading[strong_sell] = 0  # Sell\n",
    "    \n",
    "    print(\"Advanced Trading Signal Distribution:\")\n",
    "    print(f\"Sell (0): {np.sum(y_trading == 0)} ({np.sum(y_trading == 0)/len(y_trading)*100:.1f}%)\")\n",
    "    print(f\"Hold (1): {np.sum(y_trading == 1)} ({np.sum(y_trading == 1)/len(y_trading)*100:.1f}%)\")\n",
    "    print(f\"Buy (2): {np.sum(y_trading == 2)} ({np.sum(y_trading == 2)/len(y_trading)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nMarket Regime Analysis:\")\n",
    "    print(f\"High volatility periods: {np.sum(vol_regime)/len(vol_regime)*100:.1f}%\")\n",
    "    print(f\"Strong buy signals: {np.sum(strong_buy)} ({np.sum(strong_buy)/len(y_trading)*100:.1f}%)\")\n",
    "    print(f\"Strong sell signals: {np.sum(strong_sell)} ({np.sum(strong_sell)/len(y_trading)*100:.1f}%)\")\n",
    "    \n",
    "    return y_trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af405af9",
   "metadata": {},
   "source": [
    "# 5. Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pattern probabilities instead of one-hot for richer information\n",
    "pattern_probs = pattern_model.predict(x_seq)  # Soft probabilities\n",
    "print(\"Pattern probabilities shape:\", pattern_probs.shape)\n",
    "\n",
    "# Add more technical indicators as per-timestep features\n",
    "def add_more_indicators(df, lookback):\n",
    "    \"\"\"Add additional technical indicators\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # EMAs\n",
    "    df['ema_12'] = df['close'].ewm(span=12).mean()\n",
    "    df['ema_26'] = df['close'].ewm(span=26).mean()\n",
    "    \n",
    "    # Additional features if not already present\n",
    "    if 'rsi' not in df.columns:\n",
    "        df['rsi'] = calculate_rsi(df['close'], 14)\n",
    "    if 'macd' not in df.columns:\n",
    "        df['macd'], df['macd_signal'] = calculate_macd(df['close'])\n",
    "    \n",
    "    # Stochastic oscillator\n",
    "    df['stoch_k'] = ((df['close'] - df['low'].rolling(14).min()) / \n",
    "                     (df['high'].rolling(14).max() - df['low'].rolling(14).min())) * 100\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "# Add enhanced features to the same dataframe to maintain alignment\n",
    "df_enhanced = add_more_indicators(df, lookback)\n",
    "\n",
    "# Enhanced feature set\n",
    "enhanced_feature_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\",\n",
    "    \"ma_5\", \"ma_20\", \"ema_12\", \"ema_26\", \n",
    "    \"price_change\", \"volatility\", \"hl_spread\", \"oc_spread\",\n",
    "    \"rsi\", \"macd\", \"macd_signal\", \"stoch_k\", \"stoch_d\"\n",
    "]\n",
    "\n",
    "# Ensure same length by aligning with pattern predictions\n",
    "min_length = min(len(df_enhanced) - lookback, len(pattern_probs))\n",
    "print(f\"Aligning to minimum length: {min_length}\")\n",
    "\n",
    "# Generate trading signals first (now that min_length is defined)\n",
    "y_trading = generate_advanced_trading_signals(df_enhanced, lookback)\n",
    "y_trading_aligned = y_trading[:min_length]\n",
    "print(f\"Aligned trading signals shape: {y_trading_aligned.shape}\")\n",
    "\n",
    "# Get enhanced features and scale them\n",
    "x_enhanced = df_enhanced[enhanced_feature_cols].values\n",
    "scaler_enhanced = MinMaxScaler()\n",
    "x_enhanced_scaled = scaler_enhanced.fit_transform(x_enhanced)\n",
    "\n",
    "# Create sequences for CNN+LSTM (keep 3D structure) - align with pattern predictions\n",
    "x_seq_enhanced = []\n",
    "for i in range(lookback, lookback + min_length):\n",
    "    x_seq_enhanced.append(x_enhanced_scaled[i-lookback:i])\n",
    "\n",
    "x_seq_enhanced = np.array(x_seq_enhanced)\n",
    "print(\"Enhanced sequence shape:\", x_seq_enhanced.shape)\n",
    "\n",
    "# Trim pattern probabilities to match\n",
    "pattern_probs_aligned = pattern_probs[:min_length]\n",
    "print(\"Aligned pattern probabilities shape:\", pattern_probs_aligned.shape)\n",
    "\n",
    "# Combine sequences with pattern probabilities as additional features per timestep\n",
    "sequence_length, n_base_features = x_seq_enhanced.shape[1], x_seq_enhanced.shape[2]\n",
    "n_pattern_features = pattern_probs_aligned.shape[1]\n",
    "\n",
    "# Create final trading sequences\n",
    "x_trading_seq = np.zeros((min_length, sequence_length, n_base_features + n_pattern_features))\n",
    "x_trading_seq[:, :, :n_base_features] = x_seq_enhanced\n",
    "\n",
    "# Add pattern probabilities as additional features at each timestep\n",
    "for i in range(sequence_length):\n",
    "    x_trading_seq[:, i, n_base_features:] = pattern_probs_aligned\n",
    "\n",
    "print(\"Final trading sequence shape:\", x_trading_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbaf487",
   "metadata": {},
   "source": [
    "# 6. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f403e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_trading_seq, y_trading_aligned, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Test set shape:\", x_test.shape)\n",
    "print(\"Training labels distribution:\", np.bincount(y_train))\n",
    "print(\"Test labels distribution:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9014d",
   "metadata": {},
   "source": [
    "# 7. Build Trading Signal Model (CNN + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters for tuning\n",
    "sequence_length = 20  # Can tune from 5-30\n",
    "kernel_size = 5      # Can tune from 3-7\n",
    "lstm_units = 64      # Can tune from 32-128\n",
    "\n",
    "input_shape = x_train.shape[1:]  # (sequence_length, n_features)\n",
    "num_classes = 3\n",
    "\n",
    "# CNN + LSTM Model for Time Series - Simplified to reduce overfitting\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "# Simplified CNN layers - reduce complexity\n",
    "x = layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "x = layers.Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Simplified LSTM layers\n",
    "x = layers.LSTM(lstm_units//2, return_sequences=False, dropout=0.4, recurrent_dropout=0.4)(x)\n",
    "\n",
    "# Simplified dense layers\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "trading_model = models.Model(inputs, outputs)\n",
    "\n",
    "# Lower learning rate for stability\n",
    "initial_learning_rate = 0.0005\n",
    "\n",
    "trading_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "trading_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2328a14",
   "metadata": {},
   "source": [
    "# 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5066dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalance - but cap them to reduce instability\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                   classes=np.unique(y_train), \n",
    "                                   y=y_train)\n",
    "\n",
    "# Cap class weights to prevent extreme values\n",
    "max_weight = 3.0\n",
    "class_weights = np.clip(class_weights, 0.5, max_weight)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Capped class weights:\", class_weight_dict)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss', min_delta=0.001),\n",
    "    ReduceLROnPlateau(patience=8, factor=0.5, monitor='val_loss', verbose=1, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = trading_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,  # Smaller batch size for more stable gradients\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3148a9",
   "metadata": {},
   "source": [
    "# 9. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Basic evaluation\n",
    "test_loss, test_accuracy = trading_model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Detailed predictions\n",
    "y_pred = trading_model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, \n",
    "                          target_names=['Sell', 'Hold', 'Buy']))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Sell', 'Hold', 'Buy'],\n",
    "            yticklabels=['Sell', 'Hold', 'Buy'])\n",
    "plt.title('Trading Signal Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f2d22",
   "metadata": {},
   "source": [
    "# 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Model Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Model Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42365fd4",
   "metadata": {},
   "source": [
    "# 11. Walk-Forward Validation (Optional - Better than Single Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c586cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(x_data, y_data, model_builder, n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for time series data\n",
    "    Better than random split as it respects temporal order\n",
    "    \"\"\"\n",
    "    total_samples = len(x_data)\n",
    "    split_size = total_samples // n_splits\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(n_splits - 1):\n",
    "        # Progressive training set (expanding window)\n",
    "        train_end = (i + 2) * split_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + split_size\n",
    "        \n",
    "        # Split data\n",
    "        x_train_fold = x_data[:train_end]\n",
    "        y_train_fold = y_data[:train_end]\n",
    "        x_test_fold = x_data[test_start:test_end]\n",
    "        y_test_fold = y_data[test_start:test_end]\n",
    "        \n",
    "        print(f\"\\nFold {i+1}:\")\n",
    "        print(f\"Train: 0 to {train_end} ({len(x_train_fold)} samples)\")\n",
    "        print(f\"Test: {test_start} to {test_end} ({len(x_test_fold)} samples)\")\n",
    "        \n",
    "        # Build and train model\n",
    "        model = model_builder(x_train_fold.shape[1:])\n",
    "        \n",
    "        # Class weights for this fold\n",
    "        fold_class_weights = compute_class_weight('balanced', \n",
    "                                                classes=np.unique(y_train_fold), \n",
    "                                                y=y_train_fold)\n",
    "        fold_class_weight_dict = dict(enumerate(fold_class_weights))\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(x_train_fold, y_train_fold,\n",
    "                 validation_split=0.2,\n",
    "                 epochs=30,  # Reduced for faster validation\n",
    "                 batch_size=64,\n",
    "                 class_weight=fold_class_weight_dict,\n",
    "                 verbose=0)\n",
    "        \n",
    "        # Evaluate\n",
    "        _, accuracy = model.evaluate(x_test_fold, y_test_fold, verbose=0)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Fold {i+1} accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, \n",
    "                         sequence_length=20, \n",
    "                         kernel_size=5, \n",
    "                         lstm_units=64):\n",
    "    \"\"\"Model builder function for walk-forward validation\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Multi-scale CNN\n",
    "    conv1 = layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    conv2 = layers.Conv1D(32, kernel_size, activation='relu', padding='same')(inputs)\n",
    "    conv3 = layers.Conv1D(32, 7, activation='relu', padding='same')(inputs)\n",
    "    \n",
    "    x = layers.Concatenate()([conv1, conv2, conv3])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, kernel_size, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, dropout=0.3)(x)\n",
    "    x = layers.LSTM(lstm_units//2, dropout=0.3)(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run walk-forward validation (uncomment to run)\n",
    "# print(\"Running walk-forward validation...\")\n",
    "# wf_accuracies = walk_forward_validation(x_trading_seq, y_trading_aligned, create_cnn_lstm_model)\n",
    "# print(f\"\\nWalk-forward validation results:\")\n",
    "# print(f\"Mean accuracy: {np.mean(wf_accuracies):.4f} Â± {np.std(wf_accuracies):.4f}\")\n",
    "# print(f\"Individual fold accuracies: {wf_accuracies}\")\n",
    "\n",
    "print(\"Walk-forward validation function defined. Uncomment to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
