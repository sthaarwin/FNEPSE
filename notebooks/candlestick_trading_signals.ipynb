{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cceb10",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras import layers, models #type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa169c",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/merged-stock-data.csv\")\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "\n",
    "def add_advanced_technical_features(df):\n",
    "    \"\"\"Add advanced technical indicators used by professional traders\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic technical features\n",
    "    df['ma_5'] = df['close'].rolling(5).mean()\n",
    "    df['ma_20'] = df['close'].rolling(20).mean()\n",
    "    df['price_change'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['close'].rolling(10).std()\n",
    "    df['hl_spread'] = (df['high'] - df['low']) / df['close']\n",
    "    df['oc_spread'] = (df['close'] - df['open']) / df['open']\n",
    "    \n",
    "    # Advanced momentum indicators\n",
    "    df['rsi'] = calculate_rsi(df['close'], 14)\n",
    "    df['macd'], df['macd_signal'] = calculate_macd(df['close'])\n",
    "    df['bb_upper'], df['bb_lower'], df['bb_width'] = calculate_bollinger_bands(df['close'])\n",
    "    \n",
    "    # Volume-based indicators (if volume available)\n",
    "    if 'Volume' in df.columns:\n",
    "        df['volume_sma'] = df['Volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['Volume'] / df['volume_sma']\n",
    "        df['vwap'] = calculate_vwap(df)\n",
    "    \n",
    "    # Support/Resistance levels\n",
    "    df['support'], df['resistance'] = calculate_support_resistance(df)\n",
    "    \n",
    "    # Market structure\n",
    "    df['higher_high'] = calculate_higher_highs(df['high'])\n",
    "    df['lower_low'] = calculate_lower_lows(df['low'])\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD indicator\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    macd_signal = macd.ewm(span=signal).mean()\n",
    "    return macd, macd_signal\n",
    "\n",
    "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    sma = prices.rolling(period).mean()\n",
    "    std = prices.rolling(period).std()\n",
    "    upper = sma + (std * std_dev)\n",
    "    lower = sma - (std * std_dev)\n",
    "    width = (upper - lower) / sma\n",
    "    return upper, lower, width\n",
    "\n",
    "def calculate_vwap(df):\n",
    "    \"\"\"Calculate Volume Weighted Average Price\"\"\"\n",
    "    if 'Volume' in df.columns:\n",
    "        return (df['close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "def calculate_support_resistance(df, window=20):\n",
    "    \"\"\"Calculate support and resistance levels\"\"\"\n",
    "    support = df['low'].rolling(window).min()\n",
    "    resistance = df['high'].rolling(window).max()\n",
    "    return support, resistance\n",
    "\n",
    "def calculate_higher_highs(highs, period=5):\n",
    "    \"\"\"Detect higher highs pattern\"\"\"\n",
    "    return highs > highs.shift(1).rolling(period).max()\n",
    "\n",
    "def calculate_lower_lows(lows, period=5):\n",
    "    \"\"\"Detect lower lows pattern\"\"\"\n",
    "    return lows < lows.shift(1).rolling(period).min()\n",
    "\n",
    "print(\"Adding advanced technical features...\")\n",
    "df = add_advanced_technical_features(df)\n",
    "\n",
    "feature_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\",\n",
    "    \"ma_5\", \"ma_20\", \"price_change\", \"volatility\", \"hl_spread\", \"oc_spread\"\n",
    "]\n",
    "\n",
    "print(f\"Enhanced features: {len(feature_cols)} features\")\n",
    "print(f\"Data shape after feature engineering: {df.shape}\")\n",
    "\n",
    "x_raw = df[feature_cols].values\n",
    "scaler = MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x_raw)\n",
    "print(\"Features scaled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41febc13",
   "metadata": {},
   "source": [
    "# 3. Load Saved Pattern Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aafdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_model = load_model(\"../models/candlestick_cnn_lstm.h5\")\n",
    "\n",
    "lookback = 20\n",
    "\n",
    "x_seq = []\n",
    "for i in range(lookback, len(x_scaled)):\n",
    "    x_seq.append(x_scaled[i-lookback:i])\n",
    "\n",
    "x_seq = np.array(x_seq)\n",
    "\n",
    "patterns_preds = np.argmax(pattern_model.predict(x_seq), axis=1)\n",
    "print(\"predicted pattern shape : \", patterns_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d86360",
   "metadata": {},
   "source": [
    "# 4. Create Trading Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2072716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_advanced_trading_signals(df, lookback):\n",
    "    \"\"\"Generate sophisticated trading signals using multiple criteria with better balance\"\"\"\n",
    "    \n",
    "    # Multi-timeframe analysis - ensure we're not using future data\n",
    "    returns_1d = df[\"close\"].pct_change().values\n",
    "    returns_5d = df[\"close\"].pct_change(periods=5).values\n",
    "    returns_20d = df[\"close\"].pct_change(periods=20).values\n",
    "    \n",
    "    # Trim arrays to match lookback\n",
    "    returns_1d = returns_1d[lookback:]\n",
    "    returns_5d = returns_5d[lookback:]\n",
    "    returns_20d = returns_20d[lookback:]\n",
    "    \n",
    "    # More dynamic thresholds based on market conditions\n",
    "    volatility = df['volatility'].values[lookback:]\n",
    "    rsi_values = df['rsi'].values[lookback:] if 'rsi' in df.columns else np.full(len(returns_1d), 50)\n",
    "    macd_values = df['macd'].values[lookback:] - df['macd_signal'].values[lookback:] if 'macd' in df.columns else np.zeros(len(returns_1d))\n",
    "    \n",
    "    # Dynamic percentile thresholds instead of fixed ones\n",
    "    vol_high = np.percentile(volatility, 60)\n",
    "    \n",
    "    # More balanced signal criteria\n",
    "    y_trading = np.ones(len(returns_1d), dtype=int)  # Default Hold\n",
    "    \n",
    "    # BUY signals - combine multiple strong indicators\n",
    "    momentum_buy = returns_1d > np.percentile(returns_1d, 75)  # Top 25% momentum\n",
    "    trend_buy = returns_5d > np.percentile(returns_5d, 65)     # Strong 5-day trend\n",
    "    rsi_buy = (rsi_values < 40) | (rsi_values > 60)           # RSI momentum (not just oversold)\n",
    "    macd_buy = macd_values > np.percentile(macd_values, 60)    # MACD bullish\n",
    "    vol_buy = volatility < vol_high                            # Lower volatility (safer)\n",
    "    \n",
    "    # SELL signals - combine multiple weak indicators  \n",
    "    momentum_sell = returns_1d < np.percentile(returns_1d, 25)  # Bottom 25% momentum\n",
    "    trend_sell = returns_5d < np.percentile(returns_5d, 35)     # Weak 5-day trend\n",
    "    rsi_sell = (rsi_values > 70) | (rsi_values < 30)           # RSI extremes\n",
    "    macd_sell = macd_values < np.percentile(macd_values, 40)    # MACD bearish\n",
    "    vol_sell = volatility < vol_high                            # Lower volatility (safer)\n",
    "    \n",
    "    # Require multiple confirmations but make it achievable\n",
    "    buy_score = momentum_buy.astype(int) + trend_buy.astype(int) + rsi_buy.astype(int) + macd_buy.astype(int) + vol_buy.astype(int)\n",
    "    sell_score = momentum_sell.astype(int) + trend_sell.astype(int) + rsi_sell.astype(int) + macd_sell.astype(int) + vol_sell.astype(int)\n",
    "    \n",
    "    # More liberal thresholds to get better balance\n",
    "    y_trading[buy_score >= 3] = 2   # Buy if 3+ indicators agree\n",
    "    y_trading[sell_score >= 3] = 0  # Sell if 3+ indicators agree\n",
    "    \n",
    "    # Add some random perturbation to break ties and add diversity\n",
    "    np.random.seed(42)\n",
    "    tie_breaker = np.random.choice([0, 1, 2], size=len(y_trading), p=[0.15, 0.7, 0.15])\n",
    "    \n",
    "    # Only apply tie breaker where we have exactly 2 signals\n",
    "    tie_positions = (buy_score == 2) | (sell_score == 2)\n",
    "    y_trading[tie_positions] = tie_breaker[tie_positions]\n",
    "    \n",
    "    print(\"Improved Trading Signal Distribution:\")\n",
    "    print(f\"Sell (0): {np.sum(y_trading == 0)} ({np.sum(y_trading == 0)/len(y_trading)*100:.1f}%)\")\n",
    "    print(f\"Hold (1): {np.sum(y_trading == 1)} ({np.sum(y_trading == 1)/len(y_trading)*100:.1f}%)\")\n",
    "    print(f\"Buy (2): {np.sum(y_trading == 2)} ({np.sum(y_trading == 2)/len(y_trading)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nSignal Quality Analysis:\")\n",
    "    print(f\"Average buy score: {np.mean(buy_score):.2f}\")\n",
    "    print(f\"Average sell score: {np.mean(sell_score):.2f}\")\n",
    "    print(f\"High volatility periods: {np.sum(volatility > vol_high)/len(volatility)*100:.1f}%\")\n",
    "    \n",
    "    return y_trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af405af9",
   "metadata": {},
   "source": [
    "# 5. Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pattern probabilities instead of one-hot for richer information\n",
    "pattern_probs = pattern_model.predict(x_seq)  # Soft probabilities\n",
    "print(\"Pattern probabilities shape:\", pattern_probs.shape)\n",
    "\n",
    "# Add more technical indicators as per-timestep features\n",
    "def add_more_indicators(df, lookback):\n",
    "    \"\"\"Add additional technical indicators\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # EMAs\n",
    "    df['ema_12'] = df['close'].ewm(span=12).mean()\n",
    "    df['ema_26'] = df['close'].ewm(span=26).mean()\n",
    "    \n",
    "    # Additional features if not already present\n",
    "    if 'rsi' not in df.columns:\n",
    "        df['rsi'] = calculate_rsi(df['close'], 14)\n",
    "    if 'macd' not in df.columns:\n",
    "        df['macd'], df['macd_signal'] = calculate_macd(df['close'])\n",
    "    \n",
    "    # Stochastic oscillator\n",
    "    df['stoch_k'] = ((df['close'] - df['low'].rolling(14).min()) / \n",
    "                     (df['high'].rolling(14).max() - df['low'].rolling(14).min())) * 100\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def generate_balanced_trading_signals(df, lookback):\n",
    "    \"\"\"Generate more balanced trading signals using forward-looking validation\"\"\"\n",
    "    \n",
    "    # Calculate future returns for labeling (this is for training only)\n",
    "    future_returns = df['close'].pct_change(periods=5).shift(-5).values[lookback:]\n",
    "    \n",
    "    # Current indicators\n",
    "    returns_1d = df[\"close\"].pct_change().values[lookback:]\n",
    "    returns_5d = df[\"close\"].pct_change(periods=5).values[lookback:]\n",
    "    volatility = df['volatility'].values[lookback:]\n",
    "    rsi_values = df['rsi'].values[lookback:] if 'rsi' in df.columns else np.full(len(returns_1d), 50)\n",
    "    macd_values = df['macd'].values[lookback:] - df['macd_signal'].values[lookback:] if 'macd' in df.columns else np.zeros(len(returns_1d))\n",
    "    \n",
    "    # Use future returns as the primary signal with current indicators as filters\n",
    "    y_trading = np.ones(len(future_returns), dtype=int)  # Default Hold\n",
    "    \n",
    "    # Dynamic thresholds based on data distribution\n",
    "    buy_threshold = np.percentile(future_returns[~np.isnan(future_returns)], 70)  # Top 30%\n",
    "    sell_threshold = np.percentile(future_returns[~np.isnan(future_returns)], 30)  # Bottom 30%\n",
    "    \n",
    "    # Current market conditions (filters)\n",
    "    stable_vol = volatility < np.percentile(volatility, 70)  # Not too volatile\n",
    "    momentum_up = returns_1d > 0\n",
    "    momentum_down = returns_1d < 0\n",
    "    rsi_neutral = (rsi_values > 25) & (rsi_values < 75)  # Not extreme\n",
    "    \n",
    "    # Generate signals with confirmation\n",
    "    buy_conditions = (future_returns > buy_threshold) & stable_vol & rsi_neutral\n",
    "    sell_conditions = (future_returns < sell_threshold) & stable_vol & rsi_neutral\n",
    "    \n",
    "    # Apply signals\n",
    "    y_trading[buy_conditions] = 2   # Buy\n",
    "    y_trading[sell_conditions] = 0  # Sell\n",
    "    \n",
    "    # Handle NaN values (end of data)\n",
    "    nan_mask = np.isnan(future_returns)\n",
    "    y_trading[nan_mask] = 1  # Default to hold\n",
    "    \n",
    "    print(\"Balanced Trading Signal Distribution:\")\n",
    "    print(f\"Sell (0): {np.sum(y_trading == 0)} ({np.sum(y_trading == 0)/len(y_trading)*100:.1f}%)\")\n",
    "    print(f\"Hold (1): {np.sum(y_trading == 1)} ({np.sum(y_trading == 1)/len(y_trading)*100:.1f}%)\")\n",
    "    print(f\"Buy (2): {np.sum(y_trading == 2)} ({np.sum(y_trading == 2)/len(y_trading)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nThresholds used:\")\n",
    "    print(f\"Buy threshold (5-day return): {buy_threshold:.4f}\")\n",
    "    print(f\"Sell threshold (5-day return): {sell_threshold:.4f}\")\n",
    "    \n",
    "    return y_trading\n",
    "\n",
    "# Add enhanced features to the same dataframe to maintain alignment\n",
    "df_enhanced = add_more_indicators(df, lookback)\n",
    "\n",
    "# Enhanced feature set\n",
    "enhanced_feature_cols = [\n",
    "    \"open\", \"high\", \"low\", \"close\",\n",
    "    \"ma_5\", \"ma_20\", \"ema_12\", \"ema_26\", \n",
    "    \"price_change\", \"volatility\", \"hl_spread\", \"oc_spread\",\n",
    "    \"rsi\", \"macd\", \"macd_signal\", \"stoch_k\", \"stoch_d\"\n",
    "]\n",
    "\n",
    "# Ensure same length by aligning with pattern predictions\n",
    "min_length = min(len(df_enhanced) - lookback, len(pattern_probs))\n",
    "print(f\"Aligning to minimum length: {min_length}\")\n",
    "\n",
    "# Get enhanced features and scale them\n",
    "x_enhanced = df_enhanced[enhanced_feature_cols].values\n",
    "scaler_enhanced = MinMaxScaler()\n",
    "x_enhanced_scaled = scaler_enhanced.fit_transform(x_enhanced)\n",
    "\n",
    "# Create sequences for CNN+LSTM (keep 3D structure) - align with pattern predictions\n",
    "x_seq_enhanced = []\n",
    "for i in range(lookback, lookback + min_length):\n",
    "    x_seq_enhanced.append(x_enhanced_scaled[i-lookback:i])\n",
    "\n",
    "x_seq_enhanced = np.array(x_seq_enhanced)\n",
    "print(\"Enhanced sequence shape:\", x_seq_enhanced.shape)\n",
    "\n",
    "# Trim pattern probabilities to match\n",
    "pattern_probs_aligned = pattern_probs[:min_length]\n",
    "print(\"Aligned pattern probabilities shape:\", pattern_probs_aligned.shape)\n",
    "\n",
    "# Combine sequences with pattern probabilities as additional features per timestep\n",
    "sequence_length, n_base_features = x_seq_enhanced.shape[1], x_seq_enhanced.shape[2]\n",
    "n_pattern_features = pattern_probs_aligned.shape[1]\n",
    "\n",
    "# Create final trading sequences\n",
    "x_trading_seq = np.zeros((min_length, sequence_length, n_base_features + n_pattern_features))\n",
    "x_trading_seq[:, :, :n_base_features] = x_seq_enhanced\n",
    "\n",
    "# Add pattern probabilities as additional features at each timestep\n",
    "for i in range(sequence_length):\n",
    "    x_trading_seq[:, i, n_base_features:] = pattern_probs_aligned\n",
    "\n",
    "print(\"Final trading sequence shape:\", x_trading_seq.shape)\n",
    "\n",
    "# Generate better balanced trading signals\n",
    "y_trading = generate_balanced_trading_signals(df_enhanced, lookback)\n",
    "y_trading_aligned = y_trading[:min_length]\n",
    "print(f\"Aligned trading signals shape: {y_trading_aligned.shape}\")\n",
    "\n",
    "# Save the scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler_enhanced, \"../models/scaler.pkl\")\n",
    "print(\"Scaler saved to ../models/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39919746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_signals_v2(df, lookback):\n",
    "    \"\"\"\n",
    "    Create more balanced trading signals using percentile-based approach\n",
    "    \"\"\"\n",
    "    # Calculate forward returns for labeling\n",
    "    forward_returns = df['close'].pct_change(periods=3).shift(-3).values[lookback:]\n",
    "    \n",
    "    # Get current technical indicators\n",
    "    rsi = df['rsi'].values[lookback:] if 'rsi' in df.columns else np.full(len(forward_returns), 50)\n",
    "    macd = (df['macd'].values[lookback:] - df['macd_signal'].values[lookback:]) if 'macd' in df.columns else np.zeros(len(forward_returns))\n",
    "    price_change = df['price_change'].values[lookback:]\n",
    "    volatility = df['volatility'].values[lookback:]\n",
    "    \n",
    "    # Create base signals using percentiles\n",
    "    y_signals = np.ones(len(forward_returns), dtype=int)  # Default hold\n",
    "    \n",
    "    # Filter out NaN values for percentile calculation\n",
    "    valid_returns = forward_returns[~np.isnan(forward_returns)]\n",
    "    \n",
    "    if len(valid_returns) > 0:\n",
    "        # Dynamic thresholds (more aggressive for better separation)\n",
    "        buy_threshold = np.percentile(valid_returns, 75)  # Top 25%\n",
    "        sell_threshold = np.percentile(valid_returns, 25)  # Bottom 25%\n",
    "        \n",
    "        # Additional filters for signal quality\n",
    "        low_vol_mask = volatility < np.percentile(volatility, 60)  # Lower volatility periods\n",
    "        trend_up = price_change > 0\n",
    "        trend_down = price_change < 0\n",
    "        \n",
    "        # Generate signals with multiple criteria\n",
    "        strong_buy = (forward_returns > buy_threshold) & low_vol_mask\n",
    "        strong_sell = (forward_returns < sell_threshold) & low_vol_mask\n",
    "        \n",
    "        # Apply signals\n",
    "        y_signals[strong_buy] = 2  # Buy\n",
    "        y_signals[strong_sell] = 0  # Sell\n",
    "        \n",
    "        # Handle NaN values at the end\n",
    "        nan_mask = np.isnan(forward_returns)\n",
    "        y_signals[nan_mask] = 1  # Default to hold\n",
    "        \n",
    "        print(\"Balanced Signal Distribution v2:\")\n",
    "        print(f\"Sell (0): {np.sum(y_signals == 0)} ({np.sum(y_signals == 0)/len(y_signals)*100:.1f}%)\")\n",
    "        print(f\"Hold (1): {np.sum(y_signals == 1)} ({np.sum(y_signals == 1)/len(y_signals)*100:.1f}%)\")\n",
    "        print(f\"Buy (2): {np.sum(y_signals == 2)} ({np.sum(y_signals == 2)/len(y_signals)*100:.1f}%)\")\n",
    "        print(f\"Buy threshold: {buy_threshold:.4f}, Sell threshold: {sell_threshold:.4f}\")\n",
    "    \n",
    "    return y_signals\n",
    "\n",
    "# Try the improved signal generation\n",
    "print(\"\\n=== Testing Improved Signal Generation ===\")\n",
    "y_trading_v2 = create_balanced_signals_v2(df_enhanced, lookback)\n",
    "y_trading_v2_aligned = y_trading_v2[:min_length]\n",
    "\n",
    "# Use the better signal generation if it's more balanced\n",
    "signal_counts_v1 = np.bincount(y_trading_aligned, minlength=3)\n",
    "signal_counts_v2 = np.bincount(y_trading_v2_aligned, minlength=3)\n",
    "\n",
    "print(f\"\\nSignal comparison:\")\n",
    "print(f\"V1 distribution: {signal_counts_v1}\")\n",
    "print(f\"V2 distribution: {signal_counts_v2}\")\n",
    "\n",
    "# Check if v2 is more balanced (less dominated by hold signals)\n",
    "v1_balance = signal_counts_v1[1] / len(y_trading_aligned)  # Hold percentage\n",
    "v2_balance = signal_counts_v2[1] / len(y_trading_v2_aligned)  # Hold percentage\n",
    "\n",
    "if v2_balance < v1_balance and signal_counts_v2[0] > 100 and signal_counts_v2[2] > 100:\n",
    "    print(\"Using improved signal generation (v2)\")\n",
    "    y_trading_aligned = y_trading_v2_aligned\n",
    "else:\n",
    "    print(\"Using original signal generation (v1)\")\n",
    "\n",
    "print(f\"Final signal distribution: {np.bincount(y_trading_aligned, minlength=3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbaf487",
   "metadata": {},
   "source": [
    "# 6. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f403e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_trading_seq, y_trading_aligned, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Test set shape:\", x_test.shape)\n",
    "print(\"Training labels distribution:\", np.bincount(y_train))\n",
    "print(\"Test labels distribution:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional data preprocessing to prevent overfitting\n",
    "def add_noise_for_regularization(x_data, noise_factor=0.01):\n",
    "    \"\"\"Add small amount of noise to prevent overfitting\"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, x_data.shape)\n",
    "    return x_data + noise\n",
    "\n",
    "def validate_data_quality(x_data, y_data):\n",
    "    \"\"\"Check for data quality issues that could cause overfitting\"\"\"\n",
    "    print(\"Data Quality Check:\")\n",
    "    print(f\"NaN values in X: {np.isnan(x_data).sum()}\")\n",
    "    print(f\"Infinite values in X: {np.isinf(x_data).sum()}\")\n",
    "    print(f\"X data range: [{np.min(x_data):.4f}, {np.max(x_data):.4f}]\")\n",
    "    print(f\"Y unique values: {np.unique(y_data)}\")\n",
    "    \n",
    "    # Check for data leakage patterns\n",
    "    feature_stds = np.std(x_data.reshape(-1, x_data.shape[-1]), axis=0)\n",
    "    low_variance_features = np.sum(feature_stds < 0.01)\n",
    "    print(f\"Low variance features (potential data leakage): {low_variance_features}\")\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "# Validate data quality\n",
    "x_trading_seq, y_trading_aligned = validate_data_quality(x_trading_seq, y_trading_aligned)\n",
    "\n",
    "# Add small amount of noise to training data only (regularization)\n",
    "x_trading_seq_regularized = add_noise_for_regularization(x_trading_seq, noise_factor=0.005)\n",
    "print(\"Added regularization noise to training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9014d",
   "metadata": {},
   "source": [
    "# 7. Build Trading Signal Model (CNN + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Extremely simple model to prevent overfitting\n",
    "input_shape = x_train.shape[1:]  # (sequence_length, n_features)\n",
    "num_classes = 3\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Building minimal model to prevent overfitting...\")\n",
    "\n",
    "# Minimal model architecture\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "# Simple feature extraction with heavy regularization\n",
    "x = layers.Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Global average pooling instead of complex layers\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Single small dense layer\n",
    "x = layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "trading_model = models.Model(inputs, outputs)\n",
    "\n",
    "# Conservative optimizer settings\n",
    "trading_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Standard learning rate\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "trading_model.summary()\n",
    "\n",
    "# Count parameters to ensure model is small enough\n",
    "total_params = trading_model.count_params()\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "if total_params > 50000:\n",
    "    print(\"WARNING: Model might be too large and prone to overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2328a14",
   "metadata": {},
   "source": [
    "# 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5066dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check original class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(f\"Sell (0): {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Hold (1): {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Buy (2): {np.sum(y_train == 2)} ({np.sum(y_train == 2)/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# Use moderate class weights instead of SMOTE to prevent overfitting\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                   classes=np.unique(y_train), \n",
    "                                   y=y_train)\n",
    "\n",
    "# Cap class weights to prevent extreme values\n",
    "max_weight = 2.0  # Reduced to prevent overfitting\n",
    "class_weights = np.clip(class_weights, 0.5, max_weight)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Conservative class weights:\", class_weight_dict)\n",
    "\n",
    "# More aggressive callbacks to prevent overfitting\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, restore_best_weights=True, monitor='val_accuracy', min_delta=0.001),\n",
    "    ReduceLROnPlateau(patience=2, factor=0.5, monitor='val_accuracy', verbose=1, min_lr=1e-8)\n",
    "]\n",
    "\n",
    "print(\"\\nStarting training with overfitting prevention...\")\n",
    "history = trading_model.fit(\n",
    "    x_train, y_train,  # Use original data, not oversampled\n",
    "    validation_split=0.2,\n",
    "    epochs=20,  # Reduced epochs\n",
    "    batch_size=256,  # Larger batch size for better generalization\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3148a9",
   "metadata": {},
   "source": [
    "# 9. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Basic evaluation\n",
    "test_loss, test_accuracy = trading_model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Detailed predictions\n",
    "y_pred = trading_model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "trading_model.save(\"../models/trading_signals_model.h5\")\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, \n",
    "                          target_names=['Sell', 'Hold', 'Buy']))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Sell', 'Hold', 'Buy'],\n",
    "            yticklabels=['Sell', 'Hold', 'Buy'])\n",
    "plt.title('Trading Signal Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f2d22",
   "metadata": {},
   "source": [
    "# 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Model Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Model Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42365fd4",
   "metadata": {},
   "source": [
    "# 11. Walk-Forward Validation (Optional - Better than Single Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c586cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(x_data, y_data, model_builder, n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for time series data\n",
    "    Better than random split as it respects temporal order\n",
    "    \"\"\"\n",
    "    total_samples = len(x_data)\n",
    "    split_size = total_samples // n_splits\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(n_splits - 1):\n",
    "        # Progressive training set (expanding window)\n",
    "        train_end = (i + 2) * split_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + split_size\n",
    "        \n",
    "        # Split data\n",
    "        x_train_fold = x_data[:train_end]\n",
    "        y_train_fold = y_data[:train_end]\n",
    "        x_test_fold = x_data[test_start:test_end]\n",
    "        y_test_fold = y_data[test_start:test_end]\n",
    "        \n",
    "        print(f\"\\nFold {i+1}:\")\n",
    "        print(f\"Train: 0 to {train_end} ({len(x_train_fold)} samples)\")\n",
    "        print(f\"Test: {test_start} to {test_end} ({len(x_test_fold)} samples)\")\n",
    "        \n",
    "        # Build and train model\n",
    "        model = model_builder(x_train_fold.shape[1:])\n",
    "        \n",
    "        # Class weights for this fold\n",
    "        fold_class_weights = compute_class_weight('balanced', \n",
    "                                                classes=np.unique(y_train_fold), \n",
    "                                                y=y_train_fold)\n",
    "        fold_class_weight_dict = dict(enumerate(fold_class_weights))\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(x_train_fold, y_train_fold,\n",
    "                 validation_split=0.2,\n",
    "                 epochs=30,  # Reduced for faster validation\n",
    "                 batch_size=64,\n",
    "                 class_weight=fold_class_weight_dict,\n",
    "                 verbose=0)\n",
    "        \n",
    "        # Evaluate\n",
    "        _, accuracy = model.evaluate(x_test_fold, y_test_fold, verbose=0)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Fold {i+1} accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, \n",
    "                         sequence_length=20, \n",
    "                         kernel_size=5, \n",
    "                         lstm_units=64):\n",
    "    \"\"\"Model builder function for walk-forward validation\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Multi-scale CNN\n",
    "    conv1 = layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    conv2 = layers.Conv1D(32, kernel_size, activation='relu', padding='same')(inputs)\n",
    "    conv3 = layers.Conv1D(32, 7, activation='relu', padding='same')(inputs)\n",
    "    \n",
    "    x = layers.Concatenate()([conv1, conv2, conv3])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, kernel_size, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, dropout=0.3)(x)\n",
    "    x = layers.LSTM(lstm_units//2, dropout=0.3)(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run walk-forward validation (uncomment to run)\n",
    "# print(\"Running walk-forward validation...\")\n",
    "# wf_accuracies = walk_forward_validation(x_trading_seq, y_trading_aligned, create_cnn_lstm_model)\n",
    "# print(f\"\\nWalk-forward validation results:\")\n",
    "# print(f\"Mean accuracy: {np.mean(wf_accuracies):.4f} ± {np.std(wf_accuracies):.4f}\")\n",
    "# print(f\"Individual fold accuracies: {wf_accuracies}\")\n",
    "\n",
    "print(\"Walk-forward validation function defined. Uncomment to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
